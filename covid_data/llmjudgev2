import subprocess
import json
import logging
import argparse
import shutil
import os
from typing import List, Dict, Any

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


# ---------------------------------------------------------------------
# 1) Ollama Inference Function with Robust Error Handling
# ---------------------------------------------------------------------
def run_ollama_inference(
    model_name: str,
    prompt: str,
    max_tokens: int = 128,
    temperature: float = 0.7,
    timeout: int = 60  # seconds
) -> str:
    """
    Calls the `ollama generate` command for the given model with the specified prompt.
    Accumulates the generated text from the JSON output and returns it as a string.

    Raises:
        FileNotFoundError: If the Ollama CLI is not found.
        subprocess.CalledProcessError: If the subprocess returns a non-zero exit status.
        subprocess.TimeoutExpired: If the subprocess times out.
    """
    # Verify that the Ollama CLI is available
    if shutil.which("ollama") is None:
        raise FileNotFoundError("Ollama CLI not found. Please ensure it is installed and in your PATH.")

    command = [
        "ollama", "generate", model_name,
        "--prompt", prompt,
        "--max_tokens", str(max_tokens),
        "--temperature", str(temperature),
        "--no-stream",  # get the entire response in one chunk
        "--json"        # output is in JSON lines
    ]

    try:
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
    except Exception as e:
        logging.error(f"Failed to start subprocess for command: {' '.join(command)}")
        raise e

    full_response = []
    try:
        # Collect stdout and stderr with a timeout
        stdout, stderr = process.communicate(timeout=timeout)
        if process.returncode != 0:
            logging.error(f"Ollama subprocess returned non-zero exit status {process.returncode}. Stderr: {stderr}")
            raise subprocess.CalledProcessError(process.returncode, command, output=stdout, stderr=stderr)

        # Process each line of stdout
        for line in stdout.splitlines():
            line = line.strip()
            if not line:
                continue
            try:
                parsed = json.loads(line)
                if "response" in parsed:
                    full_response.append(parsed["response"])
                if parsed.get("done"):
                    break
            except json.JSONDecodeError:
                logging.warning(f"Failed to decode JSON from line: {line}")
                continue
    except subprocess.TimeoutExpired:
        process.kill()
        stdout, stderr = process.communicate()
        logging.error("Ollama subprocess timed out")
        raise
    finally:
        if process.stdout:
            process.stdout.close()
        process.wait()

    return "\n".join(full_response).strip()


# ---------------------------------------------------------------------
# 2) A Simple “Judge” Function
# ---------------------------------------------------------------------
def judge_response(prompt: str, response: str) -> Dict[str, Any]:
    """
    A toy function to judge or score the response.
    Returns a dict with 'score' and 'notes'.

    For demonstration, the score is based on response length.
    """
    resp_len = len(response)
    score = min(resp_len / 50, 10.0)  # Score up to 10 points based on length
    notes = "Longer response" if resp_len > 100 else "Short response"
    return {
        "score": round(score, 2),
        "notes": notes
    }


# ---------------------------------------------------------------------
# 3) Evaluate Prompts, Models, and Approaches from a Configuration
# ---------------------------------------------------------------------
def evaluate_config(config: Dict[str, Any], output_json: str = "judgment_results.json") -> None:
    """
    Evaluates each prompt on each model using multiple approaches (parameter sets)
    defined in the configuration. The results (including judgments) are written
    to a JSON file.
    
    The configuration should be a dict with keys:
      - "prompts": List[str]
      - "models": List[str]
      - "approaches": List[Dict] (each with keys like "name", "max_tokens", "temperature")
    """
    prompts = config.get("prompts", [])
    models = config.get("models", [])
    approaches = config.get("approaches", [{"name": "default", "max_tokens": 128, "temperature": 0.7}])

    if not prompts:
        logging.error("No prompts provided in configuration.")
        return
    if not models:
        logging.error("No models provided in configuration.")
        return

    all_results = []

    for prompt in prompts:
        for model in models:
            for approach in approaches:
                max_tokens = approach.get("max_tokens", 128)
                temperature = approach.get("temperature", 0.7)
                approach_name = approach.get("name", "default")
                logging.info(f"Evaluating prompt (first 30 chars): '{prompt[:30]}...' on model: '{model}', approach: '{approach_name}'")
                try:
                    response = run_ollama_inference(model, prompt, max_tokens=max_tokens, temperature=temperature)
                except Exception as e:
                    logging.error(f"Error during inference for model '{model}', prompt '{prompt[:30]}...', approach '{approach_name}': {e}")
                    response = f"Error: {e}"
                judgment = judge_response(prompt, response)
                result = {
                    "prompt": prompt,
                    "model_name": model,
                    "approach": approach_name,
                    "approach_params": {
                        "max_tokens": max_tokens,
                        "temperature": temperature
                    },
                    "model_response": response,
                    "judgment": judgment
                }
                all_results.append(result)

    try:
        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(all_results, f, indent=2)
        logging.info(f"Results written to {output_json}")
    except Exception as e:
        logging.error(f"Failed to write results to {output_json}: {e}")


# ---------------------------------------------------------------------
# 4) Print JSON Results as a Markdown Table
# ---------------------------------------------------------------------
def json_to_table(json_file: str) -> None:
    """
    Reads the JSON file and prints a Markdown-formatted table with columns:
      Prompt | Model | Approach | Score | Notes
    """
    try:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        logging.error(f"Failed to load JSON file {json_file}: {e}")
        return

    # Table header
    header = "| Prompt                          | Model           | Approach       | Score | Notes                   |"
    divider = "|" + "-"*(len(header)-2) + "|"
    print(header)
    print(divider)

    for item in data:
        prompt = item.get("prompt", "")
        model = item.get("model_name", "")
        approach = item.get("approach", "")
        judgment = item.get("judgment", {})
        score = judgment.get("score", "")
        notes = judgment.get("notes", "")

        # Truncate long text for readability
        prompt_disp = (prompt[:30] + "...") if len(prompt) > 30 else prompt
        model_disp = (model[:15] + "...") if len(model) > 15 else model
        approach_disp = (approach[:15] + "...") if len(approach) > 15 else approach
        notes_disp = (notes[:25] + "...") if len(notes) > 25 else notes

        print(f"| {prompt_disp:30} | {model_disp:15} | {approach_disp:15} | {score:5} | {notes_disp:25} |")


# ---------------------------------------------------------------------
# 5) Configuration Loader
# ---------------------------------------------------------------------
def load_config(config_path: str) -> Dict[str, Any]:
    """
    Loads a JSON configuration file from the given path.
    """
    if not os.path.isfile(config_path):
        raise FileNotFoundError(f"Configuration file {config_path} does not exist.")
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)
        return config
    except Exception as e:
        logging.error(f"Error loading config file {config_path}: {e}")
        raise


# ---------------------------------------------------------------------
# 6) Main Command-Line Interface
# ---------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description="Evaluate multiple prompts and models using the Ollama CLI.")
    parser.add_argument("--config", type=str, help="Path to JSON configuration file.", default=None)
    parser.add_argument("--output", type=str, help="Output JSON file name.", default="judgment_results.json")
    parser.add_argument("--table", action="store_true", help="Print the results as a Markdown table.")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging.")
    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Load configuration from file or use a default configuration
    if args.config:
        try:
            config = load_config(args.config)
        except Exception as e:
            logging.error(f"Failed to load configuration: {e}")
            return
    else:
        config = {
            "prompts": [
                "Explain why the sky is blue in one paragraph.",
                "Tell me a short story about a brave turtle."
            ],
            "models": [
                "my-llama-1b",
                "my-llama-3b"
            ],
            "approaches": [
                {"name": "default", "max_tokens": 128, "temperature": 0.7},
                {"name": "creative", "max_tokens": 256, "temperature": 0.9}
            ]
        }
        logging.info("No configuration file provided. Using default configuration.")

    # Run evaluation
    evaluate_config(config, output_json=args.output)

    # Optionally, print the results as a Markdown table
    if args.table:
        json_to_table(args.output)


if __name__ == "__main__":
    main()
